{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>NLTK (Simple Examples) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Benefits of NLP</font>\n",
    "\n",
    "As all of you know, millions of gigabytes every day are generated by blogs, social websites, and web pages.\n",
    "\n",
    "There are many companies gathering all of this data to better understand users and their passions and make appropriate changes.\n",
    "\n",
    "These data could show that the people of Brazil are happy with product A, while the people of the US are happier with product B. With NLP, this knowledge can be found instantly (i.e. a real-time result). For example, search engines are a type of NLP that give the appropriate results to the right people at the right time.\n",
    "\n",
    "But search engines are not the only implementation of natural language processing (NLP). There are a lot of even more awesome implementations out there.\n",
    "## <font color='blue'>NLP Implementations</font>\n",
    "\n",
    "These are some successful implementations of natural language processing (NLP):\n",
    "\n",
    "* Search engines like Google, Yahoo, etc. Google's search engine understands that you are a tech guy, so it shows you results related to that.\n",
    "*   Social website feeds like your Facebook news feed. The news feed algorithm understands your interests using natural language processing and shows you related ads and posts more likely than other posts.\n",
    "*    Speech engines like Apple Siri.\n",
    "*    Spam filters like Google spam filters. It's not just about your usual spam filtering; now, spam filters understand what's inside the email content and see if it's spam or not.\n",
    "\n",
    "## <font color='blue'>NLP Libraries</font>\n",
    "\n",
    "There are many open source Natural Language Processing (NLP) libraries. These are some of them:\n",
    "\n",
    "    Natural language toolkit (NLTK)\n",
    "    Apache OpenNLP\n",
    "    Stanford NLP suite\n",
    "    Gate NLP library\n",
    "\n",
    "Natural language toolkit (NLTK) is the most popular library for natural language processing (NLP). It was written in Python and has a big community behind it.\n",
    "\n",
    "NLTK also is very easy to learn, actually, it's the easiest natural language processing (NLP) library that you'll use.\n",
    "\n",
    "## <font color='blue'>Install NLTK</font>\n",
    "\n",
    "\n",
    "If you are using Windows or Linux or Mac, you can install NLTK using pip: \n",
    ">\n",
    "```Python\n",
    "pip install nltk.\n",
    "```\n",
    "\n",
    "To check if NLTK has installed correctly, you can open your Python terminal and type the following: \n",
    "```Python\n",
    "Import nltk \n",
    "```\n",
    "\n",
    "If everything goes fine, that means you've successfully installed NLTK library.\n",
    "\n",
    "Once you've installed NLTK, you should install the NLTK packages by running the following code:\n",
    "```Python\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "This will show the NLTK downloader to choose what packages need to be installed.\n",
    "You can install all packages since they all have small sizes with no problem. Now, let's start the show!\n",
    "## <font color='blue'>Tokenize Text Using Pure Python</font>\n",
    "\n",
    "First, we will grab some web page content. Then, we will analyze the text to see what the page is about. We will use the urllib module to crawl the web page:\n",
    "\n",
    "```Python\n",
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen('http://php.net/')\n",
    "\n",
    "html = response.read()\n",
    "\n",
    "print (html)\n",
    "```\n",
    "\n",
    "As you can see from the printed output, the result contains a lot of HTML tags that need to be cleaned. We can use BeautifulSoup to clean the grabbed text like this:\n",
    "\n",
    "```Python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib.request \n",
    "\n",
    "response = urllib.request.urlopen('http://php.net/') \n",
    "\n",
    "html = response.read()\n",
    "\n",
    "soup = BeautifulSoup(html,\"html5lib\")\n",
    "\n",
    "text = soup.get_text(strip=True)\n",
    "\n",
    "print (text)\n",
    "```\n",
    "\n",
    "Now, we have clean text from the crawled web page. Awesome, Right?\n",
    "\n",
    "Finally, let's convert that text into tokens by splitting the text like this:\n",
    "\n",
    "```Python\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import urllib.request \n",
    "\n",
    "response = urllib.request.urlopen('http://php.net/') \n",
    "\n",
    "html = response.read() \n",
    "\n",
    "soup = BeautifulSoup(html,\"html5lib\") \n",
    "\n",
    "text = soup.get_text(strip=True) \n",
    "\n",
    "tokens = [t for t in text.split()] \n",
    "\n",
    "print (tokens)\n",
    "```\n",
    "\n",
    "## <font color='blue'>Count Word Frequency</font>\n",
    "\n",
    "The text is much better now. Let's calculate the frequency distribution of those tokens using Python NLTK. There is a function in NLTK called FreqDist() that does the job:\n",
    "\n",
    "```Python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import nltk \n",
    "\n",
    "response = urllib.request.urlopen('http://php.net/') \n",
    "\n",
    "html = response.read() \n",
    "\n",
    "soup = BeautifulSoup(html,\"html5lib\") \n",
    "\n",
    "text = soup.get_text(strip=True) \n",
    "\n",
    "tokens = [t for t in text.split()] \n",
    "\n",
    "freq = nltk.FreqDist(tokens) \n",
    "\n",
    "for key,val in freq.items(): \n",
    "\n",
    "    print (str(key) + ':' + str(val))\n",
    "    \n",
    "```\n",
    "\n",
    "If you search the output, you'll find that the most frequent token is PHP.\n",
    "\n",
    "You can plot a graph for those tokens using plot function like this: \n",
    "\n",
    "```Python\n",
    "freq.plot(20, cumulative=False)\n",
    "```\n",
    "\n",
    "From the graph, you can be sure that this article is talking about PHP. Great! There are some words like \"the,\" \"of,\" \"a,\" \"an,\" and so on. These words are stop words. Generally, stop words should be removed to prevent them from affecting our results.\n",
    "\n",
    "## <font color='blue'>Remove Stop Words Using NLTK</font>\n",
    "\n",
    "NLTK is shipped with stop words lists for most languages. To get English stop words, you can use this code:\n",
    "\n",
    "```Python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')\n",
    "```\n",
    "\n",
    "Now, let's modify our code and clean the tokens before plotting the graph. First, we will make a copy of the list. Then, we will iterate over the tokens and remove the stop words:\n",
    "\n",
    "```Python\n",
    "\n",
    "clean_tokens = tokens[:] \n",
    "\n",
    "sr = stopwords.words('english')\n",
    "\n",
    "for token in tokens:\n",
    "\n",
    "    if token in stopwords.words('english'):\n",
    "\n",
    "        clean_tokens.remove(token)\n",
    "        \n",
    "```\n",
    "\n",
    "## <font color='blue'>The final code:</font>\n",
    "\n",
    "```Python\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import urllib.request \n",
    "\n",
    "import nltk \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "response = urllib.request.urlopen('http://php.net/') \n",
    "\n",
    "html = response.read() \n",
    "\n",
    "soup = BeautifulSoup(html,\"html5lib\") \n",
    "\n",
    "text = soup.get_text(strip=True) \n",
    "\n",
    "tokens = [t for t in text.split()] \n",
    "\n",
    "clean_tokens = tokens[:] \n",
    "\n",
    "sr = stopwords.words('english') \n",
    "\n",
    "for token in tokens: \n",
    "\n",
    "    if token in stopwords.words('english'): \n",
    "\n",
    "        clean_tokens.remove(token) \n",
    "\n",
    "freq = nltk.FreqDist(clean_tokens) \n",
    "\n",
    "for key,val in freq.items(): \n",
    "\n",
    "    print (str(key) + ':' + str(val))\n",
    "```\n",
    "\n",
    "If you check the graph now, it's better than before since no stop words on the count.\n",
    "\n",
    "```Python\n",
    "freq.plot(20,cumulative=False)\n",
    "```\n",
    "\n",
    "\n",
    "## <font color='blue'>Tokenize Text Using NLTK</font>\n",
    "\n",
    "We just saw how to split the text into tokens using the split function. Now, we will see how to tokenize the text using NLTK. Tokenizing text is important since text can't be processed without tokenization. Tokenization process means splitting bigger parts to small parts.\n",
    "\n",
    "You can tokenize paragraphs to sentences and tokenize sentences to words according to your needs. NLTK is shipped with a sentence tokenizer and a word tokenizer.\n",
    "\n",
    "Let's assume that we have a sample text like the following:\n",
    "\n",
    "\n",
    "```Python\n",
    "Hello Adam, how are you? I hope everything is going well.  Today is a good day, see you dude.\n",
    "```\n",
    "To tokenize this text to sentences, we will use sentence tokenizer:\n",
    "\n",
    "\n",
    "\n",
    "```Python\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "mytext = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" \n",
    "\n",
    "print(sent_tokenize(mytext))\n",
    "```\n",
    "\n",
    "The output is the following:\n",
    "\n",
    "```Python\n",
    "['Hello Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n",
    "```\n",
    "\n",
    "You may say, This is an easy job; I don't need to use NLTK tokenization, and I can split sentences using regular expressions since every sentence is preceded by punctuation and a space.\n",
    "\n",
    "Well, take a look at the following text:\n",
    "```Python\n",
    "Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\n",
    "```\n",
    "The word Mr. is one word by itself. OK, let's try NLTK:\n",
    "```Python\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" \n",
    "\n",
    "print(sent_tokenize(mytext))\n",
    "```\n",
    "The output looks like this:\n",
    "```Python\n",
    "['Hello Mr. Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n",
    "```\n",
    "Great! It works like charm. Let's try the word tokenizer to see how it will work:\n",
    "```Python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    "\n",
    "print(word_tokenize(mytext))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is:\n",
    "```\n",
    "['Hello', 'Mr.', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\n",
    "```\n",
    "The word Mr. is one word, as expected. NLTK uses PunktSentenceTokenizer, which is a part of the **nltk.tokenize.punkt** module. This tokenizer is trained well to work with many languages.\n",
    "## <font color='blue'>Tokenize Non-English Languages Text</font>\n",
    "\n",
    "To tokenize other languages, you can specify the language like this:\n",
    "```Python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "mytext = \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour.\"\n",
    "\n",
    "print(sent_tokenize(mytext,\"french\"))\n",
    "```\n",
    "The result will be like this:\n",
    "```\n",
    "['Bonjour M. Adam, comment allez-vous?', \"J'espère que tout va bien.\", \"Aujourd'hui est un bon jour.\"]\n",
    "```\n",
    "\n",
    "## <font color='blue'>Get Synonyms From WordNet</font>\n",
    "\n",
    "\n",
    "If you remember we installed NLTK packages using nltk.download(). One of the packages was WordNet. WordNet is a database built for natural language processing. It includes groups of synonyms and a brief definition.\n",
    "\n",
    "```Python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"pain\")\n",
    "\n",
    "print(syn[0].definition())\n",
    "\n",
    "print(syn[0].examples())\n",
    "```\n",
    "\n",
    "The result is:\n",
    "\n",
    "```\n",
    "a symptom of some physical hurt or disorder\n",
    "['the patient developed severe pain and distension']\n",
    "```\n",
    "WordNet includes a lot of definitions:\n",
    "```\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"NLP\")\n",
    "\n",
    "print(syn[0].definition())\n",
    "\n",
    "syn = wordnet.synsets(\"Python\")\n",
    "\n",
    "print(syn[0].definition())\n",
    "```\n",
    "\n",
    "The result is:\n",
    "```\n",
    "the branch of information science that deals with natural language information\n",
    "\n",
    "large Old World boas\n",
    "```\n",
    "You can use WordNet to get synonymous words like this:\n",
    "```Python\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('Computer'):\n",
    "\n",
    "    for lemma in syn.lemmas():\n",
    "\n",
    "        synonyms.append(lemma.name())\n",
    "\n",
    "print(synonyms)\n",
    "```\n",
    "The output is:\n",
    "```\n",
    "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
    "```\n",
    "\n",
    "## <font color='blue'>Get Antonyms From WordNet</font>\n",
    "\n",
    "You can get the antonyms of words the same way. All you have to do is to check the lemmas before adding them to the array.  it's an antonym or not.\n",
    "```Python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"small\"):\n",
    "\n",
    "    for l in syn.lemmas():\n",
    "\n",
    "        if l.antonyms():\n",
    "\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(antonyms)\n",
    "```\n",
    "\n",
    "The output is:\n",
    "```\n",
    "['large', 'big', 'big']\n",
    "```\n",
    "This is the power of NLTK in natural language processing.\n",
    "## <font color='blue'>NLTK Word Stemming</font>\n",
    "\n",
    "Word stemming means removing affixes from words and returning the root word. (The stem of the word working is work.) Search engines use this technique when indexing pages, so many people write different versions for the same word and all of them are stemmed to the root word.\n",
    "\n",
    "There are many algorithms for stemming, but the most used algorithm is the Porter stemming algorithm. NLTK has a class called PorterStemmer that uses this algorithm.\n",
    "```Python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "print(stemmer.stem('working'))\n",
    "```\n",
    "\n",
    "The result is: work.\n",
    "\n",
    "Clear enough!\n",
    "\n",
    "There are some other stemming algorithms, like the Lancaster stemming algorithm. The output of this algorithm shows a bit different results for few words. You can try both of them to see the results.\n",
    "## <font color='blue'>Stemming Non-English Words</font>\n",
    "\n",
    "SnowballStemmer can stem 13 languages besides the English language. The supported languages are:\n",
    "```Python\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "print(SnowballStemmer.languages)\n",
    "```\n",
    "\n",
    "```\n",
    "'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish'\n",
    "```\n",
    "You can use the stem function of the SnowballStemmer class to stem non-English words like this:\n",
    "```Python\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "french_stemmer = SnowballStemmer('french')\n",
    "\n",
    "print(french_stemmer.stem(\"French word\"))\n",
    "```\n",
    "The French commenters can tell us about the results!\n",
    "\n",
    "## <font color='blue'>Lemmatizing Words Using WordNet</font>\n",
    "\n",
    "Word lemmatizing is similar to stemming, but the difference is the result of lemmatizing is a real word. Unlike stemming, when you try to stem some words, it will result in something like this:\n",
    "\n",
    "```Python\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "print(stemmer.stem('increases'))\n",
    "```\n",
    "\n",
    "The result is: increas.\n",
    "\n",
    "Now, if we try to lemmatize the same word using NLTK WordNet, the result is correct:\n",
    "\n",
    "```Python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('increases'))\n",
    "```\n",
    "\n",
    "The result is increase.\n",
    "\n",
    "The result might end up with a synonym or a different word with the same meaning. Sometimes, if you try to lemmatize a word like the word playing, it will end up with the same word. This is because the default part of speech is nouns. To get verbs, you should specify it like this:\n",
    "```Python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "```\n",
    "The result is: play.\n",
    "\n",
    "Actually, this is a very good level of text compression. You end up with about 50% to 60% compression. The result could be a verb, noun, adjective, or adverb:\n",
    "```Python\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\")) \n",
    "\n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\")) \n",
    "\n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\")) \n",
    "\n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))\n",
    "```\n",
    "The result is:\n",
    "\n",
    "play \n",
    "\n",
    "playing \n",
    "\n",
    "playing \n",
    "\n",
    "playing\n",
    "\n",
    "## <font color='blue'>Stemming and Lemmatization Difference</font>\n",
    "\n",
    "OK, let's try stemming and lemmatization for some words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "print(stemmer.stem('stones')) \n",
    "\n",
    "print(stemmer.stem('speaking')) \n",
    "\n",
    "print(stemmer.stem('bedroom')) \n",
    "\n",
    "print(stemmer.stem('jokes')) \n",
    "\n",
    "print(stemmer.stem('lisa')) \n",
    "\n",
    "print(stemmer.stem('purple')) \n",
    "\n",
    "print('----------------------') \n",
    "\n",
    "print(lemmatizer.lemmatize('stones')) \n",
    "\n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    "\n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    "\n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    "\n",
    "print(lemmatizer.lemmatize('lisa'))\n",
    "\n",
    "print(lemmatizer.lemmatize('purple'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The result is:\n",
    "\n",
    "stone \n",
    "\n",
    "speak \n",
    "\n",
    "bedroom \n",
    "\n",
    "joke \n",
    "\n",
    "lisa \n",
    "\n",
    "purpl \n",
    "\n",
    "---------------------- \n",
    "\n",
    "stone \n",
    "\n",
    "speaking \n",
    "\n",
    "bedroom\n",
    "\n",
    "joke\n",
    "\n",
    "lisa\n",
    "\n",
    "purple\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Stemming works on words without knowing their context, which is why it has lower accuracy and is faster than lemmatization.\n",
    "\n",
    "In my opinion, lemmatizing is better than stemming. Word lemmatizing returns a real word even if it's not the same word; it could be a synonym, but at least it's a real word. Sometimes, you don't care about this level of accuracy, and all you need is speed. In this case, stemming is better.\n",
    "\n",
    "All steps we discussed in this NLP tutorial involved text preprocessing. In the future posts, we will discuss text analysis using the Python NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
